{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3f6f174",
   "metadata": {},
   "source": [
    "Setting up the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd811b1-30ef-435e-8493-eb1cc6d0fcde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import torch\n",
    "from torch.utils.data import Dataset,random_split,DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Importing the U-Net model\n",
    "from models import Unet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94166cfb",
   "metadata": {},
   "source": [
    "Image transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1388ee-b45b-40ad-abff-2a8576d622fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_transform=T.Compose([\n",
    "    T.RandomHorizontalFlip(p=1),\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c54c85a0",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd0ee9-5801-4f2a-868e-cf9b1924e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentinel1Dataset(Dataset):\n",
    "    def __init__(self, chip_info_file, transform=None, target_transform=None):\n",
    "        self.chip_df = pd.read_csv(chip_info_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chip_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_vv_path = self.chip_df.iloc[idx, 0]        \n",
    "        img_vh_path = self.chip_df.iloc[idx, 1]\n",
    "        dem_path = self.chip_df.iloc[idx, 2]\n",
    "        pwat_path = self.chip_df.iloc[idx, 3]\n",
    "        \n",
    "        with rasterio.open(img_vv_path) as src_vv:\n",
    "            img_vv = src_vv.read(1)\n",
    "            img_vv = np.clip(img_vv,-50,25)\n",
    "        with rasterio.open(img_vh_path) as src_vh:\n",
    "            img_vh = src_vh.read(1)\n",
    "            img_vh = np.clip(img_vh,-50,25)\n",
    "        with rasterio.open(dem_path) as elevation:\n",
    "            ele = elevation.read(1)\n",
    "            ele = np.clip(ele,0.0,2000.0)\n",
    "        with rasterio.open(pwat_path) as pwater:\n",
    "            wat = pwater.read(1)\n",
    "            wat = np.clip(wat,0.0,100.0)            \n",
    "            \n",
    "        # Normalization        \n",
    "        img_vv_norm = (img_vv - (-50)) / (25 - (-50))\n",
    "        img_vh_norm = (img_vh - (-50)) / (25 - (-50))\n",
    "        ele_norm = ele/2000.0\n",
    "        wat_norm = wat/100.0\n",
    "        \n",
    "        image = torch.stack((torch.from_numpy(img_vv_norm),\n",
    "                             torch.from_numpy(img_vh_norm),\n",
    "                             torch.from_numpy(ele_norm),\n",
    "                             torch.from_numpy(wat_norm),))\n",
    "            \n",
    "        label_path = self.chip_df.iloc[idx, 4]\n",
    "        with rasterio.open(label_path) as src_label:\n",
    "            label = src_label.read(1)\n",
    "        label = torch.from_numpy(label).to(torch.float32)\n",
    "        label = torch.unsqueeze(label, 0)\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5422589-8dab-4a40-9e2d-d01c956b22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_data_set1 = Sentinel1Dataset('data/chip_set1.csv')\n",
    "s1_data_set2 = Sentinel1Dataset('data/chip_set2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087de53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_data_set1_T = Sentinel1Dataset('data/chip_set1.csv', transform=s1_transform, target_transform=s1_transform)\n",
    "s1_data_set2_T = Sentinel1Dataset('data/chip_set2.csv', transform=s1_transform, target_transform=s1_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9779950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_data_set1 = torch.utils.data.ConcatDataset((s1_data_set1,s1_data_set1_T))\n",
    "s1_data_set2 = torch.utils.data.ConcatDataset((s1_data_set2,s1_data_set2_T))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73d971ce-2ed4-4f12-8499-712c76ec46e6",
   "metadata": {},
   "source": [
    "Splitting dataset into training and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator().manual_seed(42)\n",
    "generator2 = torch.Generator().manual_seed(42)\n",
    "\n",
    "s1_set1_train, s1_set1_valid  = random_split(s1_data_set1, [0.9,0.1], generator=generator1)\n",
    "s1_set2_train, s1_set2_valid  = random_split(s1_data_set2, [0.9,0.1], generator=generator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2767b7df-6b9d-4289-8fab-9b2d45f4e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset1\n",
    "print('train:',len(s1_set1_train),'validation:',len(s1_set1_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset2\n",
    "print('train:',len(s1_set2_train),'validation:',len(s1_set2_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44ccf4-473b-4883-84a4-11fb00700ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set1_dataloader = DataLoader(s1_set1_train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "validation_set1_dataloader = DataLoader(s1_set1_valid, batch_size=32, shuffle=False, pin_memory=True)\n",
    "\n",
    "train_set2_dataloader = DataLoader(s1_set2_train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "validation_set2_dataloader = DataLoader(s1_set2_valid, batch_size=32, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94039e-6280-4cd5-ad39-4ba9513c530f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset1 : Display image and label chips.\n",
    "train_features, train_labels = next(iter(train_set1_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "print(train_features.dtype)\n",
    "print(train_labels.dtype)\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "rows = len(train_features)\n",
    "cols = (train_features.shape[1]+train_labels.shape[1])\n",
    "fig, axs = plt.subplots(nrows=rows, ncols=cols, layout='constrained',figsize=(5,30))\n",
    "for i in range(rows):\n",
    "    axs[i,0].imshow(train_features[i][0],cmap='gray')\n",
    "    axs[i,0].set_title('VV', fontsize=10)\n",
    "    axs[i,1].imshow(train_features[i][1],cmap='gray')\n",
    "    axs[i,1].set_title('VH', fontsize=10)\n",
    "    axs[i,2].imshow(train_features[i][2],cmap='gray')\n",
    "    axs[i,2].set_title('dem', fontsize=10),\n",
    "    axs[i,3].imshow(train_features[i][3],cmap='gray')\n",
    "    axs[i,3].set_title('pwater', fontsize=10) ,\n",
    "    axs[i,4].imshow(train_labels[i][0],cmap='Blues')\n",
    "    axs[i,4].set_title('Water_label', fontsize=10)   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e520d0fe-0368-41fc-805f-0e24722a82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchsummary import summary\n",
    "#summary(model, (4, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer,tb_writer,epoch_number):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    last_loss,running_loss = 0.0,0.0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        # Valid mask\n",
    "        valid_pixel_mask = y.ne(255)\n",
    "        pred = pred.masked_select(valid_pixel_mask)\n",
    "        y = y.masked_select(valid_pixel_mask)\n",
    "        loss = loss_fn(pred, y)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    last_loss = running_loss/num_batches\n",
    "    print(f\"Train    : Avg loss: {last_loss:>8f} \\n\")\n",
    "    tb_writer.add_scalar('Loss/train', last_loss, epoch_number)\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319aeabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(pred, true):\n",
    "    valid_pixel_mask = true.ne(255)\n",
    "    pred_arr = pred.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "    pred_arr = pred_arr.numpy()\n",
    "    pred_arr = np.where(pred_arr>0.5,1,0)\n",
    "    true_arr = true.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "    true_arr = true_arr.numpy()\n",
    "    # Intersection and union totals\n",
    "    intersection = np.logical_and(true_arr, pred_arr)\n",
    "    union = np.logical_or(true_arr, pred_arr)   \n",
    "    iou = intersection.sum()/(union.sum()+0.0000001)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b0d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn,tb_writer,epoch_number):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    last_loss, last_iou, running_loss, running_iou = 0, 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            # Valid mask\n",
    "            valid_pixel_mask = y.ne(255)\n",
    "            pred = pred.masked_select(valid_pixel_mask)\n",
    "            y = y.masked_select(valid_pixel_mask)\n",
    "            loss = loss_fn(pred, y)\n",
    "            running_loss += loss.item()\n",
    "            iou = IoU(pred, y)\n",
    "            running_iou += iou\n",
    "\n",
    "    last_loss = running_loss/num_batches\n",
    "    last_iou = running_iou/num_batches\n",
    "\n",
    "    print(f\"Test     : \\n Accuracy IOU: {(last_iou):>8f}%, Avg loss: {last_loss:>8f} \\n\")\n",
    "    tb_writer.add_scalar('Loss/test', last_loss, epoch_number)\n",
    "    tb_writer.add_scalar('Accuracy/IOU', last_iou, epoch_number)\n",
    "    \n",
    "    return last_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8dc93ea8",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db75cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2123d49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model training on dataset 01\n",
    "model = Unet(4,1).to(device)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/s1_unet_{}'.format(timestamp))\n",
    "\n",
    "epochs = 50\n",
    "best_vloss = 1_000_000.\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_avg_loss = train(train_set1_dataloader, model, loss_fn, optimizer,writer,t+1)\n",
    "    test_avg_loss = test(validation_set1_dataloader, model, loss_fn,writer,t+1)\n",
    "    \n",
    "    writer.add_scalars('Train vs. Validation Loss',\n",
    "                    { 'Training' : train_avg_loss, 'Validation' : test_avg_loss },t + 1)\n",
    "    writer.flush()\n",
    "    \n",
    "    if test_avg_loss < best_vloss:\n",
    "        best_vloss = test_avg_loss\n",
    "        model_path = './models/model_{}_{}'.format(timestamp, t+1)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "torch.cuda.empty_cache()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be57538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model training on dataset 02\n",
    "model = Unet(4,1).to(device)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/s1_unet_{}'.format(timestamp))\n",
    "\n",
    "epochs = 100\n",
    "best_vloss = 1_000_000.\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_avg_loss = train(train_set2_dataloader, model, loss_fn, optimizer,writer,t+1)\n",
    "    test_avg_loss = test(validation_set2_dataloader, model, loss_fn,writer,t+1)\n",
    "    \n",
    "    writer.add_scalars('Train vs. Validation Loss',\n",
    "                    { 'Training' : train_avg_loss, 'Validation' : test_avg_loss },t + 1)\n",
    "    writer.flush()\n",
    "    \n",
    "    if test_avg_loss < best_vloss:\n",
    "        best_vloss = test_avg_loss\n",
    "        model_path = './models/model_{}_{}'.format(timestamp, t+1)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
